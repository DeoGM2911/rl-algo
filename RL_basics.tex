\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{makecell}

% Set the margins to 1
\usepackage[margin=1in]{geometry}

% Increase line height
\linespread{1.25}

\title{RL Basics - MDP, Value functions, Policy}
\author{Dinh Dung Tran}
\date{Dec 9, 2025}

\begin{document}
\maketitle

\section*{Disclaimer} Please note that this is a work that only reflect my understanding of the materials that
I'm learning from. It contains my own understanding and interpretations, which can obviously be wrong since I'm not the expert.
However, for those who start learning about RL, I hope this can be a helpful resource. Credit to \href{https://www.youtube.com/@aiprism1155}{AI Prism} for the amazing 
recordings of the Deep RL bootcamp. I highly recommend those who want to start learning about RL to watch 
the videos. Though, it may help to have some understanding of probability and machine learning.

%Table of summary
\tableofcontents

\newpage
\section{Markov Decision Process}

\subsection{Definition}

In short, a Markov Decision Process (MDP) is a model of an environment in which an agent can take actions to influence the state of the environment, and receive rewards or punishments based on the state of the environment.

There are 4 core components of an MDP:
    \begin{itemize}
        \item State Space $\mathcal{S}$
        \item Action Space $\mathcal{A}$
        \item Transition Probability $\Pr(s' | s, a)$
        \item Reward Function $\mathcal{R}(s, a)$
    \end{itemize}

Note that \(\mathcal{S}\) and \(\mathcal{A}\) can either be discrete or continuous spaces. For example,
the grid world environment has a discrete state space (the coordinates) and a discrete action space (up, down,
left, right). On the other hand, the cart pole environment has a continuous state space (position, angle, velocity, angular velocity)
and a continuous action space (acceleration).

For the transition probability \(\Pr(s' | s, a)\), it is a probability distribution over the next state given the current state and action.
It is a Markov property, meaning that the transition probability only depends on the current state and action, and not on the history of the environment.

Lastly, the rewards funciton is a function that maps a state, an action, and the next state the action
takes the agent to a value.

Besides the core components, there are 3 more components of an MDP that we need to fully describe our problem:
    \begin{itemize}
        \item Discount Factor $0 \leq \gamma\leq 1$ which weights how important future rewards are.
        \item Initial State $s_0 \in \mathcal{S}$
        \item Horizon $H \in \mathbb{N}$ which is the maximum amount of steps the agent can take.
    \end{itemize}

\textbf{Goal}: Given an MDP \((\mathcal{S}, \mathcal{A}, \Pr(s'|s,a), \mathcal{R}, \gamma, s_0, H)\), we wish to perform a sequence of actions (trajectory) such that
the expected sum of future rewards is maximized.

With this in mind, let's also look into the definition of a policy. In short, a policy \(\pi: \mathcal{S} \rightarrow \mathcal{A}\)
is a function that maps a state to an action. Note that, we can have stochastic policies, meaning that
\(\pi(a|s)\) is a probability distribution over actions given a state. For the sake of simplicity, let's consider deterministic
policies for now.

\newpage
\section{Value Function}

\subsection{Definition}

The value function is a function that maps a state to a value, representing the expected sum of future rewards starting from that state.
That is, its formulation takes the form:
\begin{equation}
    V_H(s) = \mathbb{E}_{\pi}\bigg[\sum_{t=0}^{H} \gamma^t \mathcal{R}(s_{t+1}, a_t, s_t)\bigg]
    \label{eq:value_function}
\end{equation}
Here, \(\pi\) means that we are taking the expected value with a given policy \(\pi\).
Now, note that we can expand the value functions as follows:
\begin{equation}
    V_H(s) = \sum_{t=0}^{H} \sum_{s_{t+1}} \Pr(s_{t+1}|s_t,a_t)\gamma^t \mathcal{R}(s_{t+1}, a_t, s_t)
    \label{eq:value_function_exp}
\end{equation}
where \(s_{t+1}\) is a possible next state by performing action \(a_t\) in state \(s_t\). Now

One can deduct from the definition of a stochastic policy that \(\Pr(s'|s_t,a_t) = \sum_{a'} \pi(a'|s_t) \Pr(s'|s_t,a')\). Substituting this into \eqref{eq:value_function_exp}, we get:
\begin{equation}
    V_H(s) = \sum_{t=0}^{H} \sum_{s_{t+1}} \sum_{a_t} \pi(a_t|s_t) \Pr(s_{t+1}|s_t,a_t)\gamma^t \mathcal{R}(s_{t+1}, a_t, s_t)
    \label{eq:value_function_exp2}
\end{equation}

However, to make the expression more readable, we will only use deterministic policy for now.

\subsection{Bellman Equation}

One thing to note is that if we are from state \(s_t\) and perform action \(a\) that leads us to state \(s_{t+1}\),
then
\begin{equation*}
    V_{H}(s_t) \text{ can be estimated by } \mathcal{R}(s_{t+1}, a_t, s_t) + \gamma V_{H-1}(s_{t+1})
\end{equation*}

One can understand the Bellman equation as a recursive relationship between the value function at time step \(t\) and the value function at time step \(t+1\).
This is because the value function at time step \(t+1\) is the expected sum of future rewards starting from state \(s_{t+1}\), and the value function at time step \(t\) is the expected sum of future rewards starting from state \(s_t\).
The \(H\) turn into \(H-1\) because we have performed an action.

Since we have a transition probability and we are maximizing, the correct (tabular) Bellman equation should be:
\begin{equation}
    V_{H}(s_t) = \max_{\substack{a_t}}\sum_{s_{t+1}} \Pr(s_{t+1}|s_t, a_t)\bigg(\mathcal{R}(s_{t+1}, a_t, s_t) + \gamma V_{H-1}(s_{t+1})\bigg)
    \label{eq:bellman_tabular}
\end{equation}

Now, if one have some prior knowledge about Markov processes, most of the time, the model will reach an
equilibrium point, meaning that the value function will converge to a fixed point. One can also view this statement
as: as the agent has infinite amount of time, it will eventually perform actions that maximize the expected sum of future rewards.

The same happens here. Consider the
case when \(H \rightarrow \infty\), then \(V_H(s) \rightarrow V^*(s)\), where \(V^*(s)\) is the value function of the state \(s\)
when the agent acts optimally (to maximize the expected sum of future rewards). Then, the Bellman equation states that:
\begin{equation}
    \forall s \in \mathcal{S}: V^*(s) = \max_{\substack{a}}\sum_{s'} \Pr(s'|s, a)\bigg(\mathcal{R}(s', a, s) + \gamma V^*(s')\bigg)
    \label{eq:bellman_eq}
\end{equation}
where \(s'\) is the next state.

\newpage
\section{Action-Value function}

\subsection{Definition}

The action-value function is a function that maps a state and an action to a value, representing the expected sum of future rewards starting from that state and action.
That is, its formulation takes the form:
\begin{equation}
    Q_H(s, a) = \mathbb{E}_{\pi}\bigg[\sum_{t=0}^{H} \gamma^t \mathcal{R}(s_{t+1}, a_t, s_t)\bigg]
    \label{eq:action_value_function}
\end{equation}
where \(a_0=a\) and \(s_0=s\). Basically, the action-value function is the same as the value function, just that the first
action is fixed. A simple relation is 
\begin{equation}
    V_H(s) = \max_{\substack{a}} Q_H(s, a)
    \label{eq:V-Q_relation}
\end{equation}

\subsection{Bellman Equation}

Similar to the value function, we also have the Bellman equation for the action-value function:
\begin{equation}
    Q_{H}(s, a) = \sum_{s'} \Pr(s'|s, a) \bigg(\mathcal{R}(s', a, s) + \gamma \max_{\substack{a'}} Q_{H-1}(s', a')\bigg)
    \label{eq:bellman_action_value_tabular}
\end{equation}
and 
\begin{equation}
    Q^*(s, a) = \sum_{s'} \Pr(s'|s, a) \bigg(\mathcal{R}(s', a, s) + \gamma \max_{\substack{a'}} Q^*(s', a')\bigg)
    \label{eq:bellman_action_value}
\end{equation}
where equation \eqref{eq:bellman_action_value} is \eqref{eq:bellman_action_value_tabular} when \(H \rightarrow \infty\).

\newpage
\section{Different approaches to RL}

There are two main flavors of RL: \textbf{Dynamic Programming} and \textbf{Policy Optimization}. The goal of 
both approaches is to find a policy that maximizes the expected sum of future rewards. However, the ways they
obtain the result are different.

\subsection{Dynamic Programming}

If you have learn Algorithms, you'll probably have some sense that based on the Bellman equation \ref{eq:bellman_tabular} and
\ref{eq:bellman_action_value_tabular}, we can use DP to solve the MDP. There are two main DP methods: 
\textbf{Policy Iteration} and \textbf{Value Iteration}. One flavor of Value Iteration is the infamous
\textbf{Q-learning}. We'll cover Q-learning and other methods in later chapters. Here, we want to focus in
the differentiation between Policy Iteration and Value Iteration.

\subsubsection{Value Iteration}

Based on equation \ref{eq:bellman_tabular}, we can initialize a DP table with 2 dimensions: states and horizon. Below is
the pseudocode for Value Iteration:

\begin{algorithm}[H]
    \caption{Value Iteration}
    \begin{algorithmic}
        \REQUIRE {Transition probability \(\Pr(s'|s, a)\), Reward function \(\mathcal{R}(s', a, s)\), Discount factor \(\gamma\), the horizon \(H\)}
        \ENSURE {Value function \(V_H(s), \forall s \in \mathcal{S}\)}
        \STATE Initialize \(V_0(s) = 0\) for all \(s \in \mathcal{S}\)
        \FOR {\(h = 0\) to \(H\)}
            \FOR {\(s \in \mathcal{S}\)}
                \STATE \(V_h(s) \leftarrow \max_{\substack{a}}\sum_{s'} \Pr(s'|s, a)\bigg(\mathcal{R}(s', a, s) + \gamma V_{h-1}(s')\bigg)\)
                \STATE \(\pi_h(s) \leftarrow \arg \max_{\substack{a}}\sum_{s'} \Pr(s'|s, a)\bigg(\mathcal{R}(s', a, s) + \gamma V_{h-1}(s')\bigg)\)
            \ENDFOR
        \ENDFOR
        \RETURN \(V_H(s), \pi_H(s)\)
    \end{algorithmic}
    \label{alg:value_iteration}
\end{algorithm}

As \(H \rightarrow \infty\), the value function will converges. There is a theorem about this, 
which I don't go into details here. One can google about it. In short, the theorem states that
algorithm \ref{alg:value_iteration} will converge to the optimal value function and 
\[\forall s \in \mathcal{S}: V^*(s) = \max_{\substack{a}}\sum_{s'} \Pr(s'|s, a)\bigg(\mathcal{R}(s', a, s) + \gamma V^*(s')\bigg)\]

One can check this file  for the implementation of Value Iteration. Note that
I vectorize the Bellman equation to make it faster.

\subsubsection{Policy Iteration}

In contrast to Value Iteration, Policy iteration is a two-step process: policy evaluation and policy improvement. Policy evaluation is the same as value iteration,
while policy improvement is the same as policy iteration.

Here, given a policy \(\pi\), we can evaluate a policy based on the resulted value function.
\begin{equation}
    V_t^\pi(s) = \sum_{s'} \Pr(s'|s, \pi(s))\bigg(\mathcal{R}(s', \pi(s), s) + \gamma V_{t-1}^\pi(s')\bigg)
\end{equation}
Note that our policy have `fixed' our action. Again, one can extend this to stochastic policy.

\begin{algorithm}[H]
    \caption{Policy Iteration}
    \begin{algorithmic}
        \REQUIRE {Transition probability \(\Pr(s'|s, a)\), Reward function \(\mathcal{R}(s', a, s)\), Discount factor \(\gamma\), number of iterations \(T\)}
        \STATE Initialize \(\pi_0\) randomly
        \STATE Compute \(V^{\pi_0}(s), \forall s \in \mathcal{S}\)
        \FOR {\(t = 1\) to \(T\)}
            \STATE Improve: \(\forall s \in \mathcal{S}: \pi_{t}(s) \leftarrow \arg \max_{\substack{a}}\sum_{s'} \Pr(s'|s, a)\bigg(\mathcal{R}(s', a, s) + \gamma V^{\pi_{t-1}}(s')\bigg)\)
            \STATE Evaluation: Compute \(V^{\pi_t}(s), \forall s \in \mathcal{S}\) 
        \ENDFOR
        \RETURN \(V_T(s), \pi_T(s)\)
    \end{algorithmic}
    \label{alg:policy_iteration}
\end{algorithm}

One can check this file for the implementation of Policy Iteration. Note that
I vectorize the Bellman equation to make it faster.

Again, there's a theorem that states that Policy Iteration will converge to the optimal policy and value function, but I won't cover it here.

\subsection{Policy Optimization}

There are two main flavors of Policy Optimization: \textbf{DFO/Evolution} and \textbf{Policy Gradients}.
We won't go into details about DFO/Evolution.

\subsubsection{Policy Gradients}

I will provide the details when we cover Policy Gradients.

\subsubsection{Others}

There is a class of methods called Actor-Critic methods. All of value iteration, policy iteration, and policy gradient are special cases of Actor-Critic methods.
In simple sense, one given policy is our actor, which interact with the environment and collect data.
We will then evaluate (critics) the policy based on the collected data and improve.

\newpage
\section{Q-Learning - Sample-based Approximation}

\subsection{Motivation and Algorithm}

A down-side with Value Iteration and Policy Iteration is that they require a complete knowledge of the environment.
That is we need to know the transition probability and reward function, which in real life, we rarely know. Furthermore,
due to the DP nature, we can only really solve problems with small state spaces and action spaces.

So, how can we deal with this? It would be nice if one have some knowledge regarding estimates, but in short, if we have
some statistics we want to estimate, we can create some `estimators' which are random variables.
An estimator is unbiased if its expected value is equal to the true value. That is if our true statistics
is \(q\) and our estimator is \(\hat{q}\), then \(\mathbb{E}[\hat{q}] = q\).

With this in minds, consider the following estimate for the value function:
\[\hat{V}(s) = \mathcal{R}(s', a, s) + \gamma V(s')\]

Note that, \(s'\) is a random variable. And thus, if we take the expectation, we will arrive at our good old
value function. Hence, this is an unbiased estimator. This inspires the following algorithm

\begin{algorithm}[H]
    \caption{Sample-based Approximation}
    \begin{algorithmic}
        \REQUIRE A way to interact with the environment which will get us the reward of an action and the next state. 
        \STATE Sample \(s' \sim \Pr(s' | s, a)\)
        \STATE \(\hat{V}(s) \leftarrow \mathcal{R}(s', a, s) + \gamma \hat{V}(s')\)
    \end{algorithmic}
\end{algorithm}

Now, it's worth noting that we're define this recursively. However, the key idea is that, if we can take a bunch of
transition samples \((s, a, s', r=\mathcal{R}(s', a, s))\), we can estimate the value function by taking the mean of the estimators.
Similarly, we can create an estimator for the action-value function \(Q(s, a)\). This gives rise to Q-Learning.

\begin{algorithm}[H]
    \caption{Q-Learning}
    \begin{algorithmic}
        \REQUIRE A way to interact with the environment which will get us the reward of an action and the next state. 
        \STATE Initialize \(Q(s, a) = 0, \forall (s, a) \in \mathcal{S} \times \mathcal{A}\)
        \STATE Initialize an starting state \(s \leftarrow s_0\)
        \FOR {\(t = 1,2,3,\ldots\) until convergence}
            \STATE Sample an action \(a\), get the next state \(s'\), and the reward \(r\)
            \IF {\(s'\) is terminal}
                \STATE Initialize a new starting state \(s \leftarrow s^*\)
                \STATE \(\texttt{target} \leftarrow r\)
            \ELSE
                \STATE \(\texttt{target} \leftarrow r + \gamma \max_{\substack{a'}} Q(s', a')\)
            \ENDIF
            \STATE \(Q(s, a) \leftarrow Q(s, a) + \alpha(t) (\texttt{target} - Q(s, a))\)
            \STATE \(s \leftarrow s'\) if not terminal else \(s \leftarrow s^*\)
        \ENDFOR
        \RETURN \(Q(s, a)\)
    \end{algorithmic}
\end{algorithm}

There are some uncertainty in the algorithm. Mainly, how do we sample actions? If always choose the action
with the highest Q-value, we will get stuck in a local optimum. If we always choose a random action, we will not
exploit the best action. To avoid this, we can use an epsilon-greedy policy.

\begin{algorithm}[H]
    \caption{\(\epsilon\)-greedy}
    \begin{algorithmic}
        \REQUIRE \(Q(s, a), s, \epsilon(t)\) where \(\epsilon(t)\) is a time-dependent value between 0 and 1.
        \STATE \(x \leftarrow \texttt{random}(0, 1)\)
        \IF {\(x < \epsilon(t)\)}
            \STATE \(a \leftarrow \texttt{random}(\mathcal{A})\)
        \ELSE
            \STATE \(a \leftarrow \arg \max_{\substack{a'}} Q(s, a')\)
        \ENDIF
        \RETURN \(a\)
    \end{algorithmic}
\end{algorithm}

Note that we want \(\epsilon(t)\) to be close to 1 initially and decreasing over time. This ensures that 
we explore initially and then exploit once the agent understand the environment. In practice, this is pretty good.

\subsection{Properties of Q-Learning}

Q-Learning is an off-policy algorithm, that is we learn the Q-value from a non-optimal policy. Furthermore, since we are iterating and improve
our estimates, this belongs to the value iteration family.

Now, to ensure convergence, we want our learning rate \(\alpha(t)\) to be close to 1 initially and decreasing over time. A general
guidelines is to choose \(\alpha(t)\) such that
\[\begin{aligned}
    \sum_{t=0}^\infty \alpha(t) = \infty & & \text{ and } & & \sum_{y=0}^{\infty} \alpha^2(t) < \infty
\end{aligned}\]

\subsection{Temporal Difference Learning}

In Q-Learning, we try to estimate the Q-value. What's about the value function? It turns out, we can
use a similar idea to estimate the value function. Of course, this is motivated by the unbiased estimator
discussion we have earlier. This is called Temporal Difference Learning.

\begin{equation}
    V_t^{\pi}(s) \leftarrow V_{t-1}^{\pi}(s) + \alpha(t) (r_t + \gamma V_{t+1}^{\pi}(s') - V_t^{\pi}(s))
    \label{eq:td-learning}
\end{equation}

We'll repeat this process \ref{eq:td-learning} until convergence. Again, we will also update our policy by
\[\pi_{t}(s) \leftarrow \arg \max_{\substack{a}} \sum_{s'} \Pr(s'|s,a) \bigg(\mathcal{R}(s', a, s) + \gamma V^{\pi_{t-1}}(s')\bigg)\]

Note that the subscript for \ref{eq:td-learning} is for different time step when evaluate the policy, while the superscript for the
policy update rule is the `outer' iteration. 
\newpage
\section{Deep Reinforcement Learning}

\subsection{Motivation}

Though Q-Learning is good, there is a major limitation with scalability. Will Q-Learning work
for continuous spaces (especially when we don't want dicretize the space)? How about very large state spaces?

Looking over to our friend supervised learning, maybe, instead of trying hard to use a table,
maybe we can learn some kind of functions that can take a state and a action as input and output a value. That function
may also take learnable parameters \(\theta\). This is called Approximate Q-Learning.

We will then use gradient descent to update the parameters \(\theta\) to minimize the loss function \(\mathcal{L}(\theta; s, a) 
= \dfrac{1}{2}(r + \gamma \max_{\substack{a'}} Q(s', a') - Q(s, a))^2\). The update rule is
\[\theta_{t} \leftarrow \theta_{t-1} - \alpha(t) \nabla_{\theta} \mathcal{L}(\theta; s, a)\]

We can see that Q-Learning is actually a special case of Approximate Q-Learning where
our function is parameterized by \(\theta \in \mathbb{R}^{|\mathcal{S}| \times |\mathcal{A}|}\).

But is this enough? Not really.

\subsection{Algorithm}

There is still a major problem: stability. If one looks at the loss function, 
we can see that our target is non-stationary (dependent on \(Q(\cdot)\)). Furthermore,
in supervised learning, we often assume that the data points are i.i.d (independently and identically distributed).
This is not the case in RL, where picking a starting point may affect the overall trajectory.
All of these problems can lead to unstable or even diverging behavior.

To address this, there are 2 basic strategies: 
\begin{itemize}
    \item Experience Replay
    \item Target Network
\end{itemize}

\subsubsection{Experience Replay}

Note that, one way to mitigate the "non-stationary" target and reduce the trajectory-dependence is to use experience replay. 
The idea is to store a fixed length trajectory, which are a sequence of transitions. Then, transitions are sampled from
the replay buffer and used for gradient descent. This methods make the data distribution more stable than the online one.

\subsubsection{Target Network}

The next solution used is a target network. The idea is to have a target network that is updated less often than the
online network. This helps to stabilize the learning process by making the target more `stationary'. 

\subsubsection{Native Deep Q-Network (DQN)}

With the two solutions in mind, we can create an algorithm that mimics Approximate Q-learning but is
much more stable. This is called Deep Q-Network (DQN).

\begin{algorithm}
    \caption{DQN}
    \begin{algorithmic}
        \REQUIRE A way to interact with the environment, number of episodes \(M\), number of time steps \(T\)
        \STATE Initialize replay buffer \(\mathcal{D}\) of length \(N\)
        \STATE Initialize the action-value function \(Q(s, a; \theta)\) for random weights \(\theta\)
        \STATE Initialize the target network \(\hat{Q}(s, a; \hat{\theta})\) with weights \(\hat{\theta}=\theta\) initially.
        \FOR{episode \(0, 1, 2, \ldots, M\)}
            \STATE Initialize the environment and get the initial state \(s_0\)
            \FOR{time step \(t=0, 1, 2, \ldots, T\)}
                \STATE Choose action \(a_t\) using \(\epsilon\)-greedy policy
                \STATE Take action \(a_t\) and observe the next state \(s_{t+1}\) and reward \(r_t\)
                \STATE Store the transition \((s_t, a_t, r_t, s_{t+1})\) in the replay buffer. One need to keep track if this is terminal to compute the target.
            \IF{the replay buffer is full} 
                \STATE Sample a batch of \((s_j, a_j, r_j, s_{j+1}) \in \mathcal{D}\) transitions from the replay buffer.
                \STATE Compute the target \(y_j = r_j + \gamma \max_{\substack{a'}} Q(s_{j+1}, a'; \hat{\theta})\) for each sample in the batch.
                \STATE Update the action-value function \(Q(s, a; \theta)\) using the sampled transitions with gradient descent.
                \STATE For every \(C\) steps, update the target network \(\hat{Q}(s, a; \hat{\theta})\) with weights \(\hat{\theta}=\theta\)
            \ENDIF
            \ENDFOR
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

Now, there are some niche details about the algorithm itself. First, the loss function
used here is the Huber loss, which penalized large errors less severely than MSE. This is
to make sure we still let the model make errors to explore the environment. The Huber loss is defined as 
\begin{equation}
    \mathcal{L}_\delta(y, \hat{y}) = \begin{cases}
        \frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| \leq \delta \\
        \delta (|y - \hat{y}| - \frac{1}{2}\delta) & \text{otherwise}
    \end{cases}
    \label{eq:huber_loss}
\end{equation}

Secondly, the optimizer we used for DQN is RMSProp or Adam, which in practice work better
than SGD. Lastly, the \(\epsilon(t)\) parameter in sampling an action is annealed over time to 
reduce exploration (\(\epsilon(0) = 1 \rightarrow \epsilon(10^6) \approx 0.05\) where \(t\) is the number of frames or steps).

\subsubsection{DQN Variants}

\paragraph{6.2.4.1 Double DQN}

Double DQN is a variant of DQN that addresses the overestimation problem in DQN. 
Now, note that when we take \(\max_{\substack{a}} Q(s', a'; \hat{\theta})\), we are using the target network to estimate the value of the next state. 
This can lead to overestimation (for some actions) if the target network is not updated often enough. To address this,
we will choose action based on the online network, but use the target network to evaluate such action.
This means that the term \(y_j - \hat{y_j}\) becomes 
\[
    r_j + \gamma Q(s', \arg \max_{\substack{a'}} Q(s', a'; \theta
    ); \hat{\theta}) - Q(s, a; \theta)
\]

The \(\arg \max\) is taken with respect to the online network (\(\theta\)), while the evaluation is done with the target network (\(\hat{\theta}\)).

\paragraph{6.2.4.2 Prioritized Experience Replay}

Prioritized Experience Replay is a variant of DQN that boosts convergence rate.

Instead of sampling data from the replay buffer u.a.r., we will sample data based on the
Bellman error of the transition. The Bellman error is defined as 
\[
    \delta_j = |y_j - \hat{y_j}| = |r_j + \gamma \max_{\substack{a'}} Q(s_{j+1}, a'; \hat{\theta}) - Q(s_j, a_j; \theta)|
\]
This will lead to faster convergence rate.

\paragraph{6.2.4.3 Dueling DQN}

Dueling DQN is a variant of DQN that also deals with the overestimation problem.

Now, instead of output the action-value function directly, we will output the value function \(V(s)\) and the advantage function \(A(s, a)\).
The advantage function, as the name suggests, show how advantagous an action is compared to some baseline, in this case, the value function.

Then, the action value function \(Q(s,a)=V(s) + A(s, a)\). However, in dueling DQN, we
compute the action-value function by
\[
    Q(s, a) = V(s) + A(s, a) - \dfrac{1}{|\mathcal{A}|} \sum_{a \in \mathcal{A}} A(s, a)
\]

\paragraph{6.2.4.4 Noisy Net for Exploration}

Noisy Net for Exploration is a variant of DQN that addresses the exploration problem in DQN.

Inspired by the idea of adding noises to make the model more robust (data augmentation), we will add noise to the parameters
so that the agent will tends to explore more than to exploit initially. Formally, the noisy linear layer is defined as 
\[
    \text{NoisyLinear}(x) = (\mu^w + \sigma^w \epsilon^w)x + (\mu^b + \sigma^b \epsilon^b)
\]
where \(\epsilon^w\) and \(\epsilon^b\) are independent random variables 
sampled from a standard normal distribution. The \(\sigma^w\) and \(\sigma^b\) are learnable parameters
that are used to control the noise.

\newpage
\section{Policy Gradients}

We will now move on to the next class of algorithms: policy gradients. To set up the framework, 
we will now use a more `flexible' policy - stochastic policy \(\pi_\theta(u|s)\). To avoid confusion, we will
now denote action by \(u\).

Compared to the old `deterministic' policy, the stochastic policy allows for a much smoother optimization problem.
That is instead of a shift from one `spiky' distribution to another, we can change the probability mass or density
of the action, which allows for much smoother optimization landscape.

Nonetheless, the ultimate goal is still to find the optimal policy that maximizes the expected return 
\(\mathbb{E}_{u \sim \pi_\theta(u|s)}[\mathcal{R}(\tau)]\) for some trajectory \(\tau\) which starts at some state \(s_0\).

\subsection{Why Policy Gradients?}

Before going into what policy gradients are, let's first think about why we use policy gradients. Compared to using
the value function \(V(s)\) and/or action-value function \(Q(s,a)\), policy gradients allows us to directly optimize the policy.
In value-based methods, we either need a dynamic models (for \(V(s)\)) or an efficient way to 
compute \(\arg \max_{\substack{a}} Q(s, a; \theta)\) for some state \(s\). However, as the problem scales larger,
these approaches aren't that good. Table \ref{tab:value_based_vs_policy_gradient} show the comparison between value-based methods and policy gradients.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{ } & \textbf{Policy Gradients} & \textbf{Dynamic Programming} \\
        \hline
        \text{Concepts} & \text{Optimize directly what we care about} & \makecell{Indirect, exploit problem structure \\ (Bellman Eq.), self-consistent} \\
        \hline
        \text{Empirical} & \makecell{Compatible with rich architectures \\(parameterized policy); versatile; auxiliry objectives} & \makecell{Exploration and off-policy learning;\\ sample-efficient (if works)} \\
        \hline
    \end{tabular}
    \caption{Comparison of Value-based (DP) Methods and Policy Gradients}
    \label{tab:value_based_vs_policy_gradient}
\end{table}

\subsection{Likelihood Ratio Policy Gradients - REINFORCE}

Let \(\tau\) be a trajectory \((s_0, u_0, s_1, u_1, \ldots, s_H, u_H)\). Then, define

\begin{equation}
    \mathcal{R}(\tau) = \sum_{t=0}^{H-1} \mathcal{R}(s_t, u_t)
    \label{eq:reward_trajectory}
\end{equation}
to be the reward of the trajectory \(\tau\). Then, the utility function is defined as 
\begin{equation}
    U(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[\mathcal{R}(\tau)] = \sum_{\tau} \Pr(\tau; \theta)\mathcal{R}(\tau)
    \label{eq:utility_function}
\end{equation}
to be the expected reward of the policy \(\pi_\theta\) over the distribution of the trajectory \(\tau\). 
The goal is to find \(\theta\) such that \(U(\theta)\) is maximized.

\subsection{Policy Gradient Theorem}

Since we're maximize \(U(\theta)\) w.r.t. \(\theta\), we can try taking the gradient of \(U(\theta)\) w.r.t. \(\theta\).

\[
    \begin{aligned}
        \nabla_\theta U(\theta) &= \nabla_\theta \sum_{\tau} \Pr(\tau; \theta)\mathcal{R}(\tau) \\
        &= \sum_{\tau} \nabla_\theta \Pr(\tau; \theta)\mathcal{R}(\tau) \\
        &= \sum_{\tau} \dfrac{\Pr(\tau; \theta)}{\Pr(\tau; \theta)}\nabla_\theta \Pr(\tau; \theta)\mathcal{R}(\tau) \\
        &= \sum_{\tau} \Pr(\tau; \theta)\dfrac{\nabla_\theta \Pr(\tau; \theta)}{\Pr(\tau; \theta)}\mathcal{R}(\tau) \\
        &= \sum_{\tau} \Pr(\tau; \theta)\nabla_\theta \log \Pr(\tau; \theta)\mathcal{R}(\tau) \\
        &= \mathbb{E}_{\tau \sim \pi_\theta}[\nabla_\theta \log \Pr(\tau; \theta)\mathcal{R}(\tau)]
    \end{aligned}
\]

This is the policy gradient theorem. It states that the gradient of the expected reward w.r.t. the policy parameters is
equal to the expected gradient of the log probability of the trajectory w.r.t. the policy parameters.

\begin{equation}
    \nabla_\theta U(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[\nabla_\theta \log \Pr(\tau; \theta)\mathcal{R}(\tau)]
    \label{eq:policy_gradient_theorem}
\end{equation}

One thing worth mentioning is the interpretation of this gradient. Now, say we have two trajectories \(\tau_1\) and
\(\tau_2\) and \(\mathcal{R}(\tau_1) > 0\) and \(\mathcal{R}(\tau_2) < 0\). 
Then, the gradient \(\nabla_\theta \log \Pr(\tau_1; \theta)\mathcal{R}(\tau_1)\)
is positive while \(\nabla_\theta \log \Pr(\tau_2; \theta)\mathcal{R}(\tau_2)\) is negative. One implication is that
the gradient increase the likelihood of the trajectory with positive reward and decrease the likelihood of the trajectory with negative reward. 
However, it's worth noting that the gradient only changes the probability, not the trajectory itself.

\subsection{REINFORCE - `Vanilla' Policy Gradients}

\subsubsection{Gradient Estimation}

With this in mind, we can use gradient descent with an approximation of the gradient \(\hat{g}  \approx \nabla_\theta U(\theta)\).
The key problem is now how to estimate \(\nabla_\theta U(\theta)\). In fact, the core issue is to estimate \(\nabla_\theta \log \Pr(\tau; \theta)\).
Let's open the gradient and see what we get
\[
    \begin{aligned}
        \nabla_\theta \log \Pr(\tau; \theta) &= \nabla_\theta \log \bigg[\prod_{t=0}^{H-1} \Pr(s_{t+1} | s_t, u_t)\pi_\theta(u_t|s_t)\bigg] \\
        &= \nabla_\theta \sum_{t=0}^{H-1} \log \Pr(s_{t+1} | s_t, u_t) + \log \pi_\theta(u_t|s_t) \\
        &= \sum_{t=0}^{H-1} \nabla_\theta \log \Pr(s_{t+1} | s_t, u_t) + \nabla_\theta \log \pi_\theta(u_t|s_t)
    \end{aligned}
\]

Now, note that the transition probability \(\Pr(s_{t+1} | s_t, u_t)\) is fixed and doesn't depend on the policy parameters \(\theta\). Thus, it's gradient w.r.t. \(\theta\) is zero. 
Therefore,
\[
    \begin{aligned}
        \nabla_\theta \log \Pr(\tau; \theta) &= \sum_{t=0}^{H-1} \nabla_\theta \log \pi_\theta(u_t|s_t)
    \end{aligned}
\]

Now, there's also a different way to arrive at this. Consider 
\begin{equation}
    U(\theta) = \mathbb{E}_{\tau \sim \theta_{\text{old}}}\bigg[\dfrac{\Pr(\tau|\theta)}{\Pr(\tau|\theta_{\text{old}})}\mathcal{R}(\tau)\bigg]
    \label{eq:utility_function_2}
\end{equation}

Here, the trajectories \(\tau\) are sampled from the old policy \(\theta_{\text{old}}\); thus, we add a correction term \(\dfrac{1}{\Pr(\tau|\theta_{\text{old}})}\).

We can now take the gradient of \(U(\theta)\) w.r.t. \(\theta\) at the point \(\theta=\theta_{\text{old}}\) to get
\begin{align}
    \nabla_\theta U(\theta) \vert_{\theta=\theta_{\text{old}}} &= \mathbb{E}_{\tau \sim \theta_{\text{old}}}\bigg[\nabla_\theta \dfrac{\Pr(\tau|\theta)\vert_{\theta=\theta_{\text{old}}}}
    {\Pr(\tau|\theta_{\text{old}})}\mathcal{R}(\tau)\bigg] \\
    &= \mathbb{E}_{\tau \sim \theta_{\text{old}}}\bigg[\nabla_\theta \log \Pr(\tau|\theta)\vert_{\theta=\theta_{\text{old}}}\mathcal{R}(\tau)\bigg]
    \label{eq:utility_gradient}
\end{align}

This is precisely the policy gradient theorem. We can see that both formulas give us the same gradient value at \(\theta=\theta_{\text{old}}\).
We can again break down the probability and arrive at the same result above.

\subsubsection{Stablizing Techniques \label{sec:stablizing_techniques}}

Now, with that in minds, there is still a problem that we need to address. Though the estimation is unbiased, it is also very noisy. To explain this,
consider the gradient estimator
\[
    \hat{g} = \dfrac{1}{m}\sum_{i=1}^{m} \nabla_\theta \log \Pr(\tau_i|\theta)\mathcal{R}(\tau_i)
\]

Note that this estimation is based on the set of trajectories \(\tau_i\). What happens if we collect some samples, and all of them have positive rewards?
Similarly, what if we collect some samples, and all of them have negative rewards? This means that the estimation is highly sensitive to noise, and one way to address
this is to collect a huge amount of data. However, computation resources aren't infinite, and we need to find a balance between the number of samples and the 
variance of the gradient estimator.

To reduce the variance, we can reduce the scale of the values that the gradient can take (\(\mathbb{V}(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2\)). 
One way to do this is to use a baseline. To be more specific, we want to `center' the rewards and ask the question: how good is this reward compared to the average reward?

\paragraph{Baseline}

Formally, the estimation becomes
\[
    \hat{g} = \dfrac{1}{m}\sum_{i=1}^{m} \nabla_\theta \log \Pr(\tau_i|\theta)(\mathcal{R}(\tau_i) - b)
\]
where \(b\) is the baseline. One key observation is that this estimator is still unbiased! Below is a proof:

\[
    \begin{aligned}
        \mathbb{E}_\tau[\nabla_\theta \log \Pr(\tau|\theta) b] &= \sum_{\tau} \Pr(\tau|\theta)\nabla_\theta \log \Pr(\tau|\theta) b \\
        &= \sum_{\tau} \Pr(\tau|\theta)\dfrac{\nabla_\theta \Pr(\tau|\theta)}{\Pr(\tau|\theta)} \mathbb{E}_\tau[b] \\
        &= \sum_{\tau} \nabla_\theta \Pr(\tau|\theta) \mathbb{E}_\tau[b] \\
        &= \nabla_\theta \Pr(\tau|\theta) \mathbb{E}_\tau[b] \\
        &= \nabla_\theta \mathbb{E}_\tau[b]
    \end{aligned}
\]

Now, if the baseline is independent of the policy (or the actions), then \(\nabla_\theta \mathbb{E}_\tau[b]=0\), and the gradient estimator is unbiased.
Note that \(b\) can depend on the states \(s\). With some hindsight, we will see that the value function \(V^\pi(s)\) is a good baseline.

\paragraph{Temporal Structure}

To further reduce the variance (reduce the scale), we can make use of the temporal structure of the trajectory.
Consider the current estimate
\[
    \begin{aligned}
        \hat{g} &= \dfrac{1}{m}\sum_{i=1}^{m} \sum_{t=0}^{H-1}\nabla_\theta \log \pi_\theta(u^{(i)}_t|s^{(i)}_t)\bigg(\sum_{k=0}^{t-1} \mathcal{R}(s^{(i)}_k, u^{(i)}_k) +
        \sum_{k=t}^{H-1} \mathcal{R}(s^{(i)}_k, u^{(i)}_k) - b\bigg) \\
    \end{aligned}
\]

Note that we can remove the term \(\sum_{k=0}^{t-1} \mathcal{R}(s^{(i)}_k, u^{(i)}_k)\) since the reward obtained before time step
\(t\) is independent of the action at time step \(t\). More formally, consider

\[
    \begin{aligned}
        \mathbb{E}_\tau\bigg[\nabla_\theta\log \pi_\theta(u_t|s_t) \sum_{k=0}^{t-1} \mathcal{R}(s^{(i)}_k, u^{(i)}_k)\bigg] &= \sum_{k=0}^{t-1} \mathcal{R}(s^{(i)}_k, u^{(i)}_k) \mathbb{E}_\tau\bigg[\nabla_\theta\log \pi_\theta(u_t|s_t)\bigg] \\
        &= \sum_{k=0}^{t-1} \mathcal{R}(s^{(i)}_k, u^{(i)}_k) \mathbb{E}_\tau\bigg[\nabla_\theta \log \pi_\theta(u_t|s_t)\bigg] \\
        &= \sum_{k=0}^{t-1} \mathcal{R}(s^{(i)}_k, u^{(i)}_k) \mathbb{E}_\tau\bigg[\dfrac{\nabla_\theta \pi_\theta(u_t|s_t)}{\pi_\theta(u_t|s_t)}\bigg] \\
        &= \sum_{k=0}^{t-1} \mathcal{R}(s^{(i)}_k, u^{(i)}_k) \nabla_\theta \mathbb{E}[1] = 0
    \end{aligned}
\]

Thus, we can simpify our gradient estimator to
\[
    \begin{aligned}
        \hat{g} &= \dfrac{1}{m}\sum_{i=1}^{m} \sum_{t=0}^{H-1}\nabla_\theta \log \pi_\theta(u^{(i)}_t|s^{(i)}_t)\bigg(\sum_{k=t}^{H-1} \mathcal{R}(s^{(i)}_k, u^{(i)}_k) - b\bigg)
    \end{aligned}
\]

But, what is this term \(\sum_{i=t}^{H-1}\mathcal{R}(s^{(i)}_k, u^{(i)}_k)\)? This is precisely the
action-value function for state \(s_t\) under policy \(\pi_\theta\), denoted as \(Q_\theta(s_t, u_t)\). Note that, for
stability, a discount factor is often added, i.e.

\begin{align}
    \hat{g} &= \dfrac{1}{m}\sum_{i=1}^{m} \sum_{t=0}^{H-1}\nabla_\theta \log \pi_\theta(u^{(i)}_t|s^{(i)}_t)\bigg(\sum_{k=t}^{H-1} \gamma^{k-t}\mathcal{R}(s^{(i)}_k, u^{(i)}_k) - b\bigg) \\
    &= \dfrac{1}{m}\sum_{i=1}^{m} \sum_{t=0}^{H-1}\nabla_\theta \log \pi_\theta(u^{(i)}_t|s^{(i)}_t)\bigg(Q^{\pi, \gamma}(s^{(i)}_t, u^{(i)}_t) - b\bigg)
    \label{eq:actor-critic}
\end{align}


Now, the last puzzle is the value of \(b\). There are some
common choices. They are

\begin{itemize}
    \item Constant baseline: \(b=\mathbb{E}[\mathcal{R}(\tau)]=\dfrac{1}{m}\sum_{i=1}^{m}\mathcal{R}(\tau_i)\)
    \item Optimal (minimum variance) function: \(b=\dfrac{\sum_i (\nabla_\theta\log \Pr(\tau_i|\theta))^2\mathcal{R}(\tau_i)}{\sum_i (\nabla_\theta\log \Pr(\tau_i|\theta))^2}\)
    \item Time-dependent function: \(b(t) = \dfrac{1}{m} \sum_{i=1}^{m}\sum_{k=t}^{H-1}\mathcal{R}(s^{(i)}_k, u^{(i)}_k)\)
    \item Value (State-dependent) function: \(b=V^\pi(s_t)\)
\end{itemize}

Among these, the value function is the one that makes sense. Why? Because \(A(s_t, u_t) = Q^\pi(s_t, u_t) - V^\pi(s_t)\), which is the advantage function, denotes 
how advantagous we are by choosing action \(u_t\) in state \(s_t\) compared to the `current' best return we know we can get from state \(s_t\).

So, the problem now boils down to estimating \(V_\phi^\pi(s)\) (\(\phi\) is the parameters
for estimating the value function). We have seen the temporal difference learning before (see equation \ref{eq:td-learning}). We still start by collecting
sample transitions \((s, u, s', r)\) and initialize \(V^\pi_{\phi_0}(s)\). However,
there is a small twist to make the training stable:

\begin{equation}
    \phi_\pi \leftarrow \arg \min_{\substack{\phi}} \dfrac{1}{m}\sum_{i=1}^{m}||r + \gamma V^\pi_\phi(s') - V^\pi_\phi(s)||_2^2 + \kappa ||\phi-\phi_i||_2^2
    \label{eq:td_learning_reinforce}
\end{equation}
The second regularization term is used to prevent overfitting and thus unstability.

Another method is to regress against empirical data. We first collect a set of trajectories \(\tau_i\), and then we try to fit a function \(V_\phi^\pi(s)\) to the data.

\begin{equation}
    \phi_\pi \leftarrow \arg \min_{\substack{\phi}} \dfrac{1}{m}\sum_{i=1}^{m}\sum_{t=0}^{H-1}\bigg(V^\pi_\phi(s^{(i)}_t) - \sum_{k=t}^{H-1}\gamma^{k-t}\mathcal{R}(s^{(i)}_k, u^{(i)}_k)\bigg)^2
    \label{eq:value_function_approx}
\end{equation}

\subsubsection{REINFORCE}

With everything in place, we are ready to see the algorithm.

\begin{algorithm}[H]
    \caption{REINFORCE - Vanilla Policy Gradient}
    \begin{algorithmic}
        \STATE Initialize policy parameters \(\theta\), baseline \(b(s_t)\)
        \FOR{each episode}
            \STATE Generate a set of trajectories \(\tau_1, \tau_2, \ldots\)
            \STATE At each time step of each trajectory, compute \(\mathcal{R}^{(i)}_t=\sum_{k=t}^{H-1}\gamma^{k-t}\mathcal{R}(s^{(i)}_k, u^{(i)}_k)\) and the
            advantage estimate \(\hat{A}^{(i)}_t = \mathcal{R}^{(i)}_t - b(s^{(i)}_t)\)
            \STATE Refit the baseline \(b(s_t)\) by minimizing \(||b(s_t)-R_t||^2\) summed over all time steps for all trajectories
            \STATE Update policy parameters \(\theta\) by using gradient estimate \(\hat{g}\)
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

Note that we use discount to increase the stability of the algorithm.

Depending on how one refit \(b(s_t)\) and compute \(\hat{A}^{(i)}_t\), the algorithm can be different.

\subsubsection{Actor-Critic Methods}

We now extends to a family of RL algorithms called Actor-Critic methods. The
\(\operatorname{REINFORCE}\) algorihtm uses Monte-Carlo methods. More specifically, it
samples trajectories and estimate the gradient of the policy using the Monte-Carlo estimate of the return.
This approach may still have high variance due to long-horizon task. To address this,
we use the idea that we have seen in DQN - an evaluating function or critic.

In short, Actor-Critic methods are RL algorithms that has two components: the actor (the policy that we used to
interact with the environment) and the critic (the evaluating function that we used to evaluate the policy).

\paragraph{General Actor-Critic algorithm}

\begin{algorithm}[H]
    \caption{General Actor-Critic algorithm}
    \begin{algorithmic}
        \STATE Initialize \(\pi_{\theta_0}\) and \(V_{\phi_0}^\pi\)
        \STATE Collect roll-outs \(\{(s_t, u_t, s_{t+1}, r_t)\}\) and compute \(\hat{Q}_i(s_t, u_t)\) for all time steps of all roll-outs
        \STATE Update: \begin{itemize}
            \item \(\theta_{T+1} \leftarrow \theta_T \alpha \dfrac{1}{m}\sum_{i=1}^{m}\sum_{t=0}^{H-1}\nabla\log\pi_{\theta_T}(u_t|s_t)(\hat{Q}_i(s_t, u_t)-V_{\phi_T}^\pi(s_t))\)
            \item \(\phi_{T+1} \leftarrow \arg\min_{\substack{\phi}}\sum_{(s_t, u_t, s_{t+1}, r_t)}||\hat{Q}_i(s_t, u_t) - V_\phi^\pi(s_t)||_2^2 + \kappa ||\phi-\phi_i||_2^2\)
        \end{itemize}
        \STATE Repeat until convergence
    \end{algorithmic}
\end{algorithm}

Some examples have been shown in the analysis in section \ref{sec:stablizing_techniques}. For example,
Instead of using \(\mathcal{R}_t\), we can use \(Q^\pi(s_t, u_t)\) (see equation \ref{eq:actor-critic}) to examine how well the policy performs.
There are some estimators for \(Q^\pi(s_t, u_t)\) that we can use, we will
see some of the ways below as we inspect some variants of the REINFORCE algorithm that implements Actor-Critic methods.


\paragraph{Asynchronous Advantage Actor-Critic: A2C and A3C} The core idea here is that 
we can express \(Q^\pi(s_t, u_t)\) as \(Q^\pi(s_t, u_t) = \mathbb{E}_\tau[r_1+\gamma r_2+\cdots+\gamma^{H-1}r_H]\).
But we have seen before that this can be simplified, with a parameter \(k\), as \(Q^\pi(s_t, u_t) = \mathbb{E}_\tau[r_1 + \gamma r_2 + \ldots \gamma r_{k-1} + \gamma^kV^\pi(s_{k+1})]\).
That is, we'll look a head \(k\) steps to estimate the value of \(Q^\pi(s_t, u_t)\). To make it clearer,
\begin{align}
    \hat{Q}^{\pi, k}(s_t, u_t) &= r_1 + \gamma r_2 + \ldots \gamma r_{k-1} + \gamma^kV^\pi(s_{k+1}) \label{eq:a3c} \\
    \mathcal{R}^{(i)}_t &= \hat{Q}^{\pi, k}(s^{(i)}_t, u^{(i)}_t) - V^\pi(s^{(i)}_t) \nonumber
\end{align}

\paragraph{Generalized Advantage Estimation: GAE} Using a somewhat similar idea to
A3C, GAE extends the idea of A3C to have an exponentially decayed average to estimate the
advantage function. To be more specific, given a parameter \(0 \leq \lambda < 1\), we have

\begin{equation}
    \hat{Q}^{\pi, \lambda}(s_t, u_t) = \sum_{k=t}^{H-1}(1-\lambda)\lambda^{k-t}\hat{Q}^{\pi, k}(s_t, u_t)
    \label{eq:gae}
\end{equation}
where \(\hat{Q}^{\pi, k}(s_t, u_t)\) is the same as in equation \ref{eq:a3c}.

\subsection{Trust Region Policy Optimization (TRPO)}

\subsubsection{REINFORCE drawbacks}

REINFORCE has a few drawbacks. First, it is not stable. To be more specific,
one can imagine this instability by looking over the over-stepping in supervised learning.
In SL, if our learning rate is too high, we may over-step and our model performs even worse after the update.
Similarly, if our update makes the policy worse, we will sample data from an even worse policy, which
leads to even worse update. This will make the training unstable. 

Second, it is not sample-efficient. Note that we collect a bunch of trajectories, but then only perform ONE
updates. This means that we need to perform a lot of episodes in order to improve the policy.

So, how can we address this?

\subsubsection{Trust Region Policy Optimization}

The core idea is to only perform a `small' step. To be more specific, we don't want
outr policy to change radically. Consider the loss function in REINFORCE:

\[
    \mathcal{L}^{PG}(\theta) = \hat{\mathbb{E}}_{t}[\log\pi_\theta(u_t|s_t)\hat{A}_t]
\]

If an action has a large empirical advantage, the update may `overpush' the probability to 1.
Similarly, if the empirical advantage is negative, the update may `overdecrease' the probability
to 0. This leads to instability with the logarithm. Now, recall equation \ref{eq:utility_function_2}.
This yields the same gradient that we use in REINFORCE. Of course, this doesn't change anything yet.
However, notice this means we can use importance sampling. 

Now, to the main idea: how do we incorporate the idea that the policy shouldn't change much? If one recalls,
in statistics, we have a powerful tool called KL-divergence. Basically, it quantifies how
different two probability distributions are from each other. From this, the problem is now an
optimization problem:

\[
    \begin{aligned}
        \text{maximize}_{\substack{\theta}} && \hat{\mathbb{E}}_t\bigg[\dfrac{\pi_\theta(u_t|s_t)}{\pi_{\theta_\text{old}}(u_t|s_t)}\hat{A}_t\bigg]\\
        \text{subject to }&& D_{KL}(\pi_\theta(\cdot|s_t)\|\pi_{\theta_\text{old}}(\cdot|s_t)) \leq \delta
    \end{aligned}
\]

That is, we now add the constraint that the KL-divergence between the old policy and the new policy should be small.
Then, we have our algorithm:

\begin{algorithm}[H]
    \caption{Trust Region Policy Optimization}
    \begin{algorithmic}
        \STATE Initialize \(\pi_{\theta_0}\)
        \STATE Collect roll-outs \(\{(s_t, u_t, s_{t+1}, r_t)\}\) and compute \(\hat{A}_t\) for all time steps of all roll-outs
        \STATE Find \(\theta\) which maximizes \(\hat{\mathbb{E}}_t\bigg[\dfrac{\pi_\theta(u_t|s_t)}{\pi_{\theta_\text{old}}(u_t|s_t)}\hat{A}_t\bigg]\)
    subjected to \(D_{KL}(\pi_\theta(\cdot|s_t)\|\pi_{\theta_\text{old}}(\cdot|s_t)) \leq \delta\)
        \STATE Repeat until convergence
    \end{algorithmic}
\end{algorithm}

There are a number of ways one can solve the optimization problem and one can search about this online. I won't go into
details here (Check `Natural Policy Gradient' or `Conjugate Gradient' for more details). 

There are two prominent variants of TRPO: KFAC, which leads to ACKTR, and the penalized KL.
For the first one, I recommend reading the paper. For the second one, the idea is quite simple - adding the constraint
to the objective:
\[
    \begin{aligned}
        \text{maximize}_{\substack{\theta}} && \hat{\mathbb{E}}_t\bigg[\dfrac{\pi_\theta(u_t|s_t)}{\pi_{\theta_\text{old}}(u_t|s_t)}\hat{A}_t - \beta D_{KL}(\pi_\theta(\cdot|s_t)\|\pi_{\theta_\text{old}}(\cdot|s_t))\bigg]
    \end{aligned}
\]
where \(\beta\) is a hyperparameter that controls how much we care about the KL-divergence. There are also
methods to solve this, one of the is the linear-quadratic approximation. This leads to a different TRPO algorithm

\begin{algorithm}[H]
    \caption{KL-penalized TRPO}
    \begin{algorithmic}
        \STATE Initialize \(\pi_{\theta_0}\)
        \STATE Collect roll-outs \(\{(s_t, u_t, s_{t+1}, r_t)\}\) and compute \(\hat{A}_t\) for all time steps of all roll-outs
        \STATE Find \(\theta\) which maximizes \(\hat{\mathbb{E}}_t\bigg[\dfrac{\pi_\theta(u_t|s_t)}{\pi_{\theta_\text{old}}(u_t|s_t)}\hat{A}_t - \beta D_{KL}(\pi_\theta(\cdot|s_t)\|\pi_{\theta_\text{old}}(\cdot|s_t))\bigg]\)
        \STATE If the KL-divergence is too large, increase \(\beta \leftarrow 1.5\beta\). If it's too low, decrease \(\beta \leftarrow \beta/1.5\)
        \STATE Repeat until convergence
    \end{algorithmic}
\end{algorithm}
The coefficients \(1.5\) and \(\dfrac{1}{1.5}\) are well-tuned hyperparameters found by the original authors.  

Now, as a reality check, note that:
\begin{itemize}
    \item If we remove the constraint (or the penalty), we have our policy iteration (start with some policy, evaluate, improve, repeat)
    \item If we replace the KL-divergence penalty with L2-regularization, we arrive at our REINFORCE algorithm.
\end{itemize}

\subsubsection{Proximal Policy Optimization}

Note that the idea in TRPO is great, however, solving the optimization problem is not
easy and fast. To make the implementation easier (and faster), we have Proximal Policy Optimization (PPO).
The idea is very staightforward. Instead of have a KL-penalty or constraint to keep the policies close to each other,
we instead clip the value of the ratio \(r_t(\theta) = \dfrac{\pi_\theta(u_t|s_t)}{\pi_{\theta_\text{old}}(u_t|s_t)}\). More specifically,
the loss is 

\begin{equation}
    \mathcal{L}^{CLIP}(\theta) = \hat{\mathbb{E}}_t[\min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)]
    \label{eq:clip_loss}
\end{equation}
where \(\epsilon\) is a hyperparameter. Now, the full loss function does include an entropy 
term for exploration and a value function term for stability. The loss in that case is
\begin{equation}
    \mathcal{L}^{PPO}(\theta) = \hat{\mathbb{E}}_t\bigg[\min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)\bigg] + c_1\hat{\mathbb{E}}_t[(V_\theta(s_t) - R_t)^2] + c_2S_{\pi_\theta}(s_t)
    \label{eq:ppo_loss}
\end{equation}
where \(R_t\) is the return at time \(t\) and \(c_1, c_2\) are hyperparameters.

\begin{algorithm}[H]
    \caption{PPO}
    \begin{algorithmic}
        \STATE Initialize \(\pi_{\theta_0}\)
        \STATE Collect roll-outs \(\{(s_t, u_t, s_{t+1}, r_t)\}\) and compute \(\hat{A}_t\) for all time steps of all roll-outs
        \STATE Sample some batches of data from the roll-outs.
        \STATE Find \(\theta\) which maximizes \(\hat{\mathbb{E}}_t\bigg[\min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)\bigg] + c_1\hat{\mathbb{E}}_t[(V_\theta(s_t) - R_t)^2] + c_2S_{\pi_\theta}(s_t)\). This can be done with SGD.
        \STATE Repeat until convergence
    \end{algorithmic}
\end{algorithm}

Note that we now use the data much more efficient and the implementation is rather straight forward.

\subsubsection{Group Relative Policy Optimization}

While we 're still talking about PPO, let's talk about a variant called Group Relative Policy Optimization (GRPO).
Basically, PPO still has the drawback of other older policy optimization algorithm: the baseline is expensive to compute.
GRPO try to solve this by instead of using a value function approximation baseline, it uses the average return of some groups of trajectories.
This is much clearer when we talk about finetuning LLMs. Now, imagine the state being the prompt/question in some dataset.
Then our trajectory is the output of the model. Then, our data for RL is obtained by sampling a batch of questions from
the dataset, then generate some outputs for each question, and do something with them. To make it clearer, let's first see
the algorithm itself:

\begin{algorithm}[H]
    \caption{Iterative Group Relative Policy Optimization}
    \begin{algorithmic}
        \REQUIRE reward model \(R_\phi\), reference model \(\pi_{\text{ref}}\), policy \(\pi_{\theta_0}\), dataset \(\mathcal{D}\)
        \STATE Initialize \(\pi_{\theta_0}\), \(\pi_{\text{ref}}\), and \(\pi_{\theta_{\text{old}}}\)
        \FOR{some number of epochs}
        \FOR{some number of iterations}
        \STATE Sample a batch of questions \(\{q_1, q_2, \ldots, q_N\}\) from \(\mathcal{D}\)
        \STATE Generate \(G\) outputs \(o_1, o_2, \ldots, o_G\) for each question based on \(\pi_{\theta_{\text{old}}}\).
        \STATE For each question \(q_i\), compute the reward \(r_j\) for each output \(o_j\) based on \(R_\phi\). 
        \STATE Compute the advantage \(A_j\) for each output \(o_j\): \(A_j = \dfrac{r_j - \mu_i}{\sigma_i}\) where \(\mu_i\) and \(\sigma_i\) are the mean and standard deviation of the rewards of the outputs for question \(q_i\).
        \STATE Maximize the following objective:
        \[
            \hat{\mathbb{E}}_{q\sim\mathcal{D}, o_j \sim \pi_{\theta_\text{old}}}
            \bigg[
                    \dfrac{1}{G}\sum_{j=1}^{G}
                    \min\bigg(\dfrac{\pi_{\theta_\text{old}}(o_j|q_i)}{\pi_{\text{ref}}(o_j|q_i)}A_j,
                    \text{clip}\bigg(\dfrac{\pi_{\theta_\text{old}}(o_j|q_i)}{\pi_{\text{ref}}(o_j|q_i)}, 1-\epsilon, 1+\epsilon\bigg)A_j
                \bigg)
                - \beta \hat{D}_{KL}(\pi_{\theta_\text{old}}\|\pi_{\text{ref}})
            \bigg]
        \] using SGD or Adam.
        \ENDFOR
        \STATE Update \(\pi_{\theta_\text{old}} \leftarrow \pi_{\theta}\)
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

Now, a few clarification on the algorithm. First, we use a different objective
compared to PPO. Note that part of it is actually still \(\mathcal{L}^{PPO}(\theta)\). However,
we also add a KL-divergence penalty to ensure the policies are to a reference model. All of these is for
the goal of stablizing training since the generated output may be long, which means there're a lot of noises.

Second, we use a different baseline. Instead of using a value function approximation, we use the average
reward of the outputs for each question. This is much cheaper to compute and still achieve good results empirically.
As you can tell, this is also why this is called `group relative.' We also normalize the advantage for
stability.

Lastly, we approximate the KL-divergence by:
\[
    \hat{D}_{KL}(\pi_{\theta_\text{old}}\|\pi_{\text{ref}}) = \hat{\mathbb{E}}_{q\sim\mathcal{D}, o_j \sim \pi_{\theta_\text{old}}}\bigg[\dfrac{\pi_{\theta_\text{old}}(o_j|q_i)}{\pi_{\text{ref}}(o_j|q_i)} - \log \dfrac{\pi_{\theta_\text{old}}(o_j|q_i)}{\pi_{\text{ref}}(o_j|q_i)} - 1\bigg]
\] 
Note that the reference model \(\pi_{\text{ref}}\) is different from \(\pi_{\theta_\text{old}}\). Please note that this method is
mostly used to finetune LLMs.
\end{document}